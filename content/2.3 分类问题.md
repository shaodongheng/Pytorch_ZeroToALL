## 2.3 分类问题

### 2.3.1 逻辑回归（Logistic）

不讲公式和原理直接上代码：

```python
import torch
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# 设定随机种子
torch.manual_seed(2017)

# 从 data.txt 中读入点
with open('./data.txt', 'r') as f:   #打开数据集文件
    data_list = [i.split('\n')[0].split(',') for i in f.readlines()]
    data = [(float(i[0]), float(i[1]), float(i[2])) for i in data_list]

# 标准化
x0_max = max([i[0] for i in data])
x1_max = max([i[1] for i in data])
data = [(i[0]/x0_max, i[1]/x1_max, i[2]) for i in data]

x0 = list(filter(lambda x: x[-1] == 0.0, data)) # 选择第一类的点，filter函数过滤满足条件的data，
x1 = list(filter(lambda x: x[-1] == 1.0, data)) # 选择第二类的点

plot_x0 = [i[0] for i in x0] #取出第一类点的第一维特征
plot_y0 = [i[1] for i in x0]
plot_x1 = [i[0] for i in x1]
plot_y1 = [i[1] for i in x1]

plt.plot(plot_x0, plot_y0, 'ro', label='x_0')
plt.plot(plot_x1, plot_y1, 'bo', label='x_1')
plt.legend(loc='best')
```

![image-20200428172028999](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200428172028999.png)

```python
np_data = np.array(data, dtype='float32') # 转换成 numpy array
x_data = torch.from_numpy(np_data[:, 0:2]) # 转换成 Tensor, 大小是 [100, 2]
y_data = torch.from_numpy(np_data[:, -1]).unsqueeze(1) # 转换成 Tensor，大小是 [100, 1]

# 定义 sigmoid 函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 画出 sigmoid 的图像

plot_x = np.arange(-10, 10.01, 0.01)
plot_y = sigmoid(plot_x)

plt.plot(plot_x, plot_y, 'r')
```

![image-20200428183042465](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200428183042465.png)

```python
x_data = Variable(x_data)
y_data = Variable(y_data)

import torch.nn.functional as F  #包含sigmoid函数的模块

# 定义 logistic 回归模型
w = Variable(torch.randn(2, 1), requires_grad=True) 
b = Variable(torch.zeros(1), requires_grad=True)

def logistic_regression(x):
    return F.sigmoid(torch.mm(x, w) + b)
```

因为在模型中学习到的参数$$w_{1},w_{2},b $$实际上是构成一条直线：
$$
w_{1}x + w_{2}y +b =0
$$
以这条直线为界，上方是一类，下方是另一类。我们可以画出来：

```python
# 画出参数更新之前的结果
w0 = w[0].data[0]
w1 = w[1].data[0]
b0 = b.data[0]

plot_x = np.arange(0.2, 1, 0.01)
plot_y = (-w0 * plot_x - b0) / w1

plt.plot(plot_x, plot_y, 'g', label='cutting line')
plt.plot(plot_x0, plot_y0, 'ro', label='x_0')
plt.plot(plot_x1, plot_y1, 'bo', label='x_1')
plt.legend(loc='best')
```

![image-20200428183144083](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200428183144083.png)

我们接下来计算损失（交叉熵）：
$$
loss = -(y * log(\hat{y}) + (1 - y) * log(1 - \hat{y}))
$$

```python
# 计算loss
def binary_loss(y_pred, y):
    logits = (y * y_pred.clamp(1e-12).log() + (1 - y) * (1 - y_pred).clamp(1e-12).log()).mean()  #torch.clamp（input, min, max, out=None）限制值得范围到min和max之间
    return -logits

y_pred = logistic_regression(x_data)
loss = binary_loss(y_pred, y_data)
print(loss)

# 自动求导并更新参数
loss.backward()
w.data = w.data - 0.1 * w.grad.data
b.data = b.data - 0.1 * b.grad.data

# 算出一次更新之后的loss
y_pred = logistic_regression(x_data)
loss = binary_loss(y_pred, y_data)
print(loss)

```

这样手动的参数更新方式明显不适合大规模参数的情况，那我们来更自动地更新：这就是 PyTorch 中的优化器 torch.optim。

使用 `torch.optim` 需要另外一个数据类型，就是 `nn.Parameter`，这个本质上和 Variable 是一样的，只不过 `nn.Parameter` 默认是要求梯度的，而 Variable 默认是不求梯度的。

使用 `torch.optim.SGD` 可以使用梯度下降法来更新参数。

将参数 w 和 b 放到 `torch.optim.SGD` 中之后，说明一下学习率的大小，就可以使用 `optimizer.step()` 来更新参数了，比如下面我们将参数传入优化器，学习率设置为 1.0。

```python
# 使用 torch.optim 更新参数
from torch import nn
w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

def logistic_regression(x):
    return F.sigmoid(torch.mm(x, w) + b)

optimizer = torch.optim.SGD([w, b], lr=1.)

# 进行 1000 次更新
import time

start = time.time()
for e in range(1000):
    # 前向传播
    y_pred = logistic_regression(x_data)
    loss = binary_loss(y_pred, y_data) # 计算 loss
    # 反向传播
    optimizer.zero_grad() # 使用优化器将梯度归 0
    loss.backward()
    optimizer.step() # 使用优化器来更新参数
    # 计算正确率
    mask = y_pred.ge(0.5).float()  #torch.ge(input, other, out=None),比较input>=other，返回True，否则False
    acc = (mask == y_data).sum().data[0] / y_data.shape[0]
    if (e + 1) % 200 == 0:
        print('epoch: {}, Loss: {:.5f}, Acc: {:.5f}'.format(e+1, loss.data[0], acc))
during = time.time() - start
print()
print('During Time: {:.3f} s'.format(during))
```

输出：

![image-20200428183402318](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200428183402318.png)

```python
# 画出更新之后的结果
w0 = w[0].data[0]
w1 = w[1].data[0]
b0 = b.data[0]

plot_x = np.arange(0.2, 1, 0.01)
plot_y = (-w0 * plot_x - b0) / w1

plt.plot(plot_x, plot_y, 'g', label='cutting line')
plt.plot(plot_x0, plot_y0, 'ro', label='x_0')
plt.plot(plot_x1, plot_y1, 'bo', label='x_1')
plt.legend(loc='best')
```

![image-20200428183441334](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200428183441334.png)

此时效果已经很好了。

这里我们自己写loss函数，其实我们可以调用pytorth的自带的损失函数。

```python
# 使用自带的loss
criterion = nn.BCEWithLogitsLoss() # 将 sigmoid 和 loss 写在一层，有更快的速度、更好的稳定性，无需再写sigmoid函数了

w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

def logistic_reg(x):
    return torch.mm(x, w) + b

optimizer = torch.optim.SGD([w, b], 1.)

y_pred = logistic_reg(x_data)
loss = criterion(y_pred, y_data)  #调用损失函数
print(loss.data)

# 同样进行 1000 次更新

start = time.time()
for e in range(1000):
    # 前向传播
    y_pred = logistic_reg(x_data)
    loss = criterion(y_pred, y_data)
    # 反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # 计算正确率
    mask = y_pred.ge(0.5).float()
    acc = (mask == y_data).sum().data[0] / y_data.shape[0]
    if (e + 1) % 200 == 0:
        print('epoch: {}, Loss: {:.5f}, Acc: {:.5f}'.format(e+1, loss.data[0], acc))

during = time.time() - start
print()
print('During Time: {:.3f} s'.format(during))
```

![image-20200428183555669](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200428183555669.png)

可以看到速度更快了一些。这只是一个小网络，迭代次数也少，所以如果在大网络上是用自带损失，会有更明显的速度提升，一般也更具稳定性。





再来一个：

```python
import torch
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from torch import nn
# 设定随机种子
torch.manual_seed(2017)


# 从 data.txt 中读入点
with open('./data.txt', 'r') as f:
    data_list = [i.split('\n')[0].split(',') for i in f.readlines()]
    data = [(float(i[0]), float(i[1]), float(i[2])) for i in data_list]

# 标准化
x0_max = max([i[0] for i in data])
x1_max = max([i[1] for i in data])
data = [(i[0]/x0_max, i[1]/x1_max, i[2]) for i in data]

np_data = np.array(data, dtype='float32') # 转换成 numpy array
x_data = torch.from_numpy(np_data[:, 0:2]) # 转换成 Tensor, 大小是 [100, 2]
y_data = torch.from_numpy(np_data[:, -1]).unsqueeze(1) # 转换成 Tensor，大小是 [100, 1]

x0 = list(filter(lambda x: x[-1] == 0.0, data)) # 选择第一类的点
x1 = list(filter(lambda x: x[-1] == 1.0, data)) # 选择第二类的点

plot_x0 = [i[0] for i in x0]
plot_y0 = [i[1] for i in x0]
plot_x1 = [i[0] for i in x1]
plot_y1 = [i[1] for i in x1]

plt.plot(plot_x0, plot_y0, 'ro', label='x_0')
plt.plot(plot_x1, plot_y1, 'bo', label='x_1')
plt.legend(loc='best')
```

![image-20200430145907305](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200430145907305.png)



```python
#定义模型
class LogisticRegression(nn.Module):
    def __init__(self):
        super(LogisticRegression, self).__init__()
        self.lr = nn.Linear(2,1) #线性函数，输入2维，输出1维
        self.sm = nn.Sigmoid()  #直接调用pytorch的sigmoid函数
        
    def forward(self,x):   
        x= self.lr(x)
        x = self.sm(x)
        return x
    
logitic_model = LogisticRegression()
if torch.cuda.is_available():
    logitic_model.cuda()
    
criterion = nn.BCELoss() #二分类损失函数
optimizer = torch.optim.SGD(logitic_model.parameters(),lr = 1e-3, momentum=0.9)

for epoch in range(50000):
    if torch.cuda.is_available():
        x = Variable(x_data).cuda()
        y = Variable(y_data).cuda()
    else:
        x = Variable(x_data)
        y = Variable(y_data)
        
    out = logitic_model(x)
    loss = criterion(out,y)
    print_loss = loss.data
    mask = out.ge(0.5).float() #
    correct = (mask == y).sum()  #预测结果与真实标签一致的个数
    #print(correct.data.cpu().numpy(),x.size(0))
    acc = correct.data.cpu().numpy()/x.size(0)  #求精度
    #print(acc)
    
    optimizer.zero_grad() #梯度归零
    loss.backward()  #反向传播
    optimizer.step() #更新参数
    
    if (epoch+1)%1000 == 0:
        print('*'*10)
        print('epoch {}'.format(epoch+1))
        print('loss is {:.4f}'.format(print_loss))
        print('acc is {:.4f}'.format(acc))
```



![image-20200503203430403](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200503203430403.png)

```python
# 画出更新之后的结果
w0,w1 = logitic_model.lr.weight[0] #模型参数
w0 = w0.data.cpu()
w1 = w1.data.cpu()
b = logitic_model.lr.bias.data[0].cpu()

plot_x = np.arange(0.2, 1, 0.01)
plot_y = (-w0 * plot_x - b) / w1

plt.plot(plot_x, plot_y, 'g', label='cutting line')
plt.plot(plot_x0, plot_y0, 'ro', label='x_0')
plt.plot(plot_x1, plot_y1, 'bo', label='x_1')
plt.legend(loc='best')
```

![image-20200503203521973](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200503203521973.png)

事实上，Logistic回归只是一个使用了Sigmoid作为激活函数的一层神经网络。

### 2.3.2 激活函数

1. Sigmoid



2. Tanh

3. Relu

4. Leak Relu

5. Maxout

   



