## 2.4 BP



### 2.4.1 全连接层二分类

```python
import torch
import numpy as np
from torch import nn
from torch.autograd import Variable
import torch.nn.functional as F

import matplotlib.pyplot as plt
%matplotlib inline


def plot_decision_boundary(model, x, y):
    # Set min and max values and give it some padding
    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) #生成点坐标
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()]) #np.ravel()多维数组降为一维
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) #画等高线
    plt.ylabel('x2')
    plt.xlabel('x1')
    plt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral)
    
    
np.random.seed(1)   
m = 400 # 样本数量
N = int(m/2) # 每一类的点的个数
D = 2 # 维度
x = np.zeros((m, D))
y = np.zeros((m, 1), dtype='uint8') # label 向量，0 表示红色，1 表示蓝色
a = 4

for j in range(2):
    ix = range(N*j,N*(j+1))
    t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta 取N个随机数
    r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius
    x[ix] = np.c_[r*np.sin(t), r*np.cos(t)] #np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等
    y[ix] = j
    
plt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral) #画散点图
```

![image-20200504131723195](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200504131723195.png)

先尝试Logistic回归来分类：

```python
x = torch.from_numpy(x).float() #numpy数组转化成tensor
y = torch.from_numpy(y).float()

w = nn.Parameter(torch.randn(2, 1)) #参数初始化（2，1）
b = nn.Parameter(torch.zeros(1)) #

optimizer = torch.optim.SGD([w, b], 1e-1)

def logistic_regression(x):
    return torch.mm(x, w) + b

criterion = nn.BCEWithLogitsLoss() #损失函数

for e in range(100):
    out = logistic_regression(Variable(x))
    loss = criterion(out, Variable(y)) #计算损失
    optimizer.zero_grad() #梯度归零
    loss.backward()
    optimizer.step()
    if (e + 1) % 20 == 0:
        print('epoch: {}, loss: {}'.format(e+1, loss.data))
        
def plot_logistic(x):
    x = Variable(torch.from_numpy(x).float())
    out = torch.sigmoid(logistic_regression(x))
    out = (out > 0.5) * 1
    return out.data.numpy()
    
plot_decision_boundary(lambda x: plot_logistic(x), x.numpy(), y.numpy())
plt.title('logistic regression')
```

![image-20200504150259766](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200504150259766.png)

逻辑回归并没有很好地分类。这是因为逻辑回归是一个线性的分类器，从图上看就是用一条直线来划分。

我们再使用神经网络。

```python
# 定义两层神经网络的参数
w1 = nn.Parameter(torch.randn(2, 4) * 0.01) # 输入2维，输出4维
b1 = nn.Parameter(torch.zeros(4)) #偏置值

w2 = nn.Parameter(torch.randn(4, 1) * 0.01) 
b2 = nn.Parameter(torch.zeros(1))

# 定义模型
def two_network(x):
    x1 = torch.mm(x, w1) + b1
    x1 = F.tanh(x1) # 使用 PyTorch 自带的 tanh 激活函数
    x2 = torch.mm(x1, w2) + b2
    return x2

optimizer = torch.optim.SGD([w1, w2, b1, b2], 1.)

criterion = nn.BCEWithLogitsLoss()

# 我们训练 10000 次
for e in range(10000):
    out = two_network(Variable(x))
    loss = criterion(out, Variable(y))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if (e + 1) % 1000 == 0:
        print('epoch: {}, loss: {}'.format(e+1, loss.data))
        
        
def plot_network(x):
    x = Variable(torch.from_numpy(x).float())
    x1 = torch.mm(x, w1) + b1
    x1 = F.tanh(x1)
    x2 = torch.mm(x1, w2) + b2
    out = F.sigmoid(x2)
    out = (out > 0.5) * 1
    return out.data.numpy()

plot_decision_boundary(lambda x: plot_network(x), x.numpy(), y.numpy())
plt.title('2 layer network')
```

![image-20200506093949128](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200506093949128.png)

由于激活函数的存在，神经网络是一个非线性的分类器。



### 2.4.2 Sequential 

对于前面的线性回归模型、 Logistic回归模型和神经网络，我们在构建的时候定义了需要的参数。这对于比较小的模型是可行的，但是对于大的模型，比如100 层的神经网络，这个时候再去手动定义参数就显得非常麻烦，所以 PyTorch 提供了两个模块来帮助我们构建模型，一个是Sequential，一个是 Module。

Sequential 允许我们构建序列化的模块，而 Module 是一种更加灵活的模型定义方式，我们下面分别用 Sequential 和 Module 来定义上面的神经网络。

```python
# Sequential
seq_net = nn.Sequential(
    nn.Linear(2, 4), # PyTorch 中的线性层，wx + b
    nn.Tanh(),
    nn.Linear(4, 1)  #注意没有sigmoid层，一般和损失函数合用
)

# 序列模块可以通过索引访问每一层

seq_net[0] # 第一层

# 打印出第一层的权重

w0 = seq_net[0].weight
print(w0)

# 通过 parameters 可以取得模型的参数
param = seq_net.parameters()

# 定义优化器
optim = torch.optim.SGD(param, 1.)

# 我们训练 10000 次
for e in range(10000):
    out = seq_net(Variable(x))
    loss = criterion(out, Variable(y))
    optim.zero_grad()
    loss.backward()
    optim.step()
    if (e + 1) % 1000 == 0:
        print('epoch: {}, loss: {}'.format(e+1, loss.data))
        
def plot_seq(x):
    out = F.sigmoid(seq_net(Variable(torch.from_numpy(x).float()))).data.numpy()
    out = (out > 0.5) * 1
    return out

plot_decision_boundary(lambda x: plot_seq(x), x.numpy(), y.numpy())
plt.title('sequential')

```

![image-20200506101218616](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200506101218616.png)

### 2.4.3 保存模型

主要有两种方式。

1. 将参数和模型保存在一起

```python
# 将参数和模型保存在一起
torch.save(seq_net, 'save_seq_net.pth')
```

```python
# 读取保存的模型
seq_net1 = torch.load('save_seq_net.pth')
print(seq_net1)
```

![image-20200506101442772](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200506101442772.png)

```python
print(seq_net1[0].weight)
```

![image-20200506101520095](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200506101520095.png)

2. 只保存模型参数

   ```python
    #保存模型参数
   torch.save(seq_net.state_dict(), 'save_seq_net_params.pth')
   ```

   通过上面的方式，我们保存了模型的参数，如果要重新读入模型的参数，首先我们需要重新定义一次模型，接着重新读入参数

   ```python
   seq_net2 = nn.Sequential(
       nn.Linear(2, 4),
       nn.Tanh(),
       nn.Linear(4, 1)
   )
   
   seq_net2.load_state_dict(torch.load('save_seq_net_params.pth'))
   
   print(seq_net2)
   ```

   

![image-20200506101658062](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200506101658062.png)

```python
print(seq_net2[0].weight)
```

![image-20200506101719055](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200506101719055.png)

推荐使用第二种方式

### 2.4.4 Module

```python
#Module模板
class 网络名字(nn.Module):
    def __init__(self, 一些定义的参数):
        super(网络名字, self).__init__()
        self.layer1 = nn.Linear(num_input, num_hidden)
        self.layer2 = nn.Sequential(...)
        ...

       # 定义需要用的网络层

    def forward(self, x): # 定义前向传播
        x1 = self.layer1(x)
        x2 = self.layer2(x)
        x = x1 + x2
        ...
        return x
```

注意的是，Module 里面也可以使用 Sequential，同时 Module 非常灵活，具体体现在 forward 中，如何复杂的操作都能直观的在 forward 里面执行。

用Module来实现神经网络：

```python
class module_net(nn.Module):
    def __init__(self, num_input, num_hidden, num_output):
        super(module_net, self).__init__()
        self.layer1 = nn.Linear(num_input, num_hidden)
        
        self.layer2 = nn.Tanh()
        
        self.layer3 = nn.Linear(num_hidden, num_output)
        
    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        return x
    
mo_net = module_net(2, 4, 1)
# 访问模型中的某层可以直接通过名字

# 第一层
l1 = mo_net.layer1
print(l1)

# 打印出第一层的权重
print(l1.weight)

# 定义优化器
optim = torch.optim.SGD(mo_net.parameters(), 1.)

# 我们训练 10000 次
for e in range(10000):
    out = mo_net(Variable(x))
    loss = criterion(out, Variable(y))
    optim.zero_grad()
    loss.backward()
    optim.step()
    if (e + 1) % 1000 == 0:
        print('epoch: {}, loss: {}'.format(e+1, loss.data))
        
 # 保存模型
torch.save(mo_net.state_dict(), 'module_net.pth')

```

### 2.4.5 深度BP神经网络

**mnist 数据集**:训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员，一共有 60000 张图片。 测试集(test set) 也是同样比例的手写数字数据，一共有 10000 张图片。每张图片大小是 28 x 28 的灰度图.

```python
import numpy as np
import torch
from torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据

from torch import nn
from torch.autograd import Variable

import matplotlib.pyplot as plt
%matplotlib inline

# 使用内置函数下载 mnist 数据集
train_set = mnist.MNIST('./data', train=True, download=True) #下训练集
test_set = mnist.MNIST('./data', train=False, download=True)  #下测试集

a_data, a_label = train_set[0]

plt.imshow(a_data，cmap ='gray')  #查看图像，灰度图
plt.show()
```

![image-20200506161607351](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200506161607351.png)

```python

print(a_label)  #标签5

a_data = np.array(a_data, dtype='float32')  #转化为numpy数组
print(a_data.shape) #（28，28）

print(a_data) #输出矩阵，矩阵中0就表示黑色，255表示白色

def data_tf(x):
    x = np.array(x, dtype='float32') / 255
    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到
    x = x.reshape((-1,)) # 拉平
    x = torch.from_numpy(x)
    return x

train_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换
test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)


a, a_label = train_set[0]
print(a.shape) 
print(a_label)

from torch.utils.data import DataLoader
# 使用 pytorch 自带的 DataLoader 定义一个数据迭代器
train_data = DataLoader(train_set, batch_size=64, shuffle=True)
test_data = DataLoader(test_set, batch_size=128, shuffle=False)

a, a_label = next(iter(train_data))

# 打印出一个批次的数据大小
print(a.shape)
print(a_label.shape)

# 使用 Sequential 定义 4 层神经网络
net = nn.Sequential(
    nn.Linear(784, 400),
    nn.ReLU(),
    nn.Linear(400, 200),
    nn.ReLU(),
    nn.Linear(200, 100),
    nn.ReLU(),
    nn.Linear(100, 10)
)

print(net)

# 定义 loss 函数
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), 1e-1) # 使用随机梯度下降，学习率 0.1


# 开始训练
losses = []
acces = []
eval_losses = []
eval_acces = []

for e in range(20):
    train_loss = 0
    train_acc = 0
    net.train()
    for im, label in train_data:
        im = Variable(im)
        label = Variable(label)
        # 前向传播
        out = net(im)
        loss = criterion(out, label)
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # 记录误差
        train_loss += loss.item()
        # 计算分类的准确率
        _, pred = out.max(1)
        num_correct = (pred == label).sum().item()
        acc = num_correct / im.shape[0]
        train_acc += acc
        
    losses.append(train_loss / len(train_data))
    acces.append(train_acc / len(train_data))
    # 在测试集上检验效果
    eval_loss = 0
    eval_acc = 0
    net.eval() # 将模型改为预测模式
    for im, label in test_data:
        im = Variable(im)
        label = Variable(label)
        out = net(im)
        loss = criterion(out, label)
        # 记录误差
        eval_loss += loss.item()
        # 记录准确率
        _, pred = out.max(1)
        num_correct = (pred == label).sum().item()
        acc = num_correct / im.shape[0]
        eval_acc += acc
        
    eval_losses.append(eval_loss / len(test_data))
    eval_acces.append(eval_acc / len(test_data))
    print('epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}, Eval Loss: {:.6f}, Eval Acc: {:.6f}'
          .format(e, train_loss / len(train_data), train_acc / len(train_data), 
                     eval_loss / len(test_data), eval_acc / len(test_data)))
    

plt.title('train loss')
plt.plot(np.arange(len(losses)), losses)

plt.plot(np.arange(len(acces)), acces)
plt.title('train acc')

plt.plot(np.arange(len(eval_losses)), eval_losses)
plt.title('test loss')

plt.plot(np.arange(len(eval_acces)), eval_acces)
plt.title('test acc')

```

![image-20200507110225189](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200507110225189.png)

![image-20200507110238992](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200507110238992.png)

![image-20200507110248481](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200507110248481.png)

![image-20200507110304422](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200507110304422.png)

再来一个：

```python
import torch
from torch import nn, optim
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import datasets,transforms


class Batch_Net(nn.Module):
    def __init__(self,in_dim,n_hidden_1,n_hidden_2,out_dim):
        super(Batch_Net,self).__init__()
        self.layer1 = nn.Sequential(nn.Linear(in_dim,n_hidden_1),
                                   nn.BatchNorm1d(n_hidden_1),nn.ReLU(True)) #用nn.Sequential()将网络的层组合到一起，作为self.layer
        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1,n_hidden_2),      #这里将nn.Linear（），nn.BatchNorm1d（），nn.ReLU（）组合为一层
                                   nn.BatchNorm1d(n_hidden_2),nn.ReLU(True)) #nn.BatchNorm1d（）：批标准化，一般放在全连接层的后面，激活函数前面
        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim))  #最后一层不添加激活函数
        
        
    def forward(self,x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        
        return x
    

batch_size = 64
learning_rate = 1e-2
num_epoches = 20

#
data_tf = transforms.Compose(
    [transforms.ToTensor(),  #将图片转换成Pytorch中处理的对象Tensor，在转化过程中PyTorch自动将图片标准化了，也就是说Tensor的范围是0-1.
    transforms.Normalize([0.5],[0.5])]) #第一个参数是均值，第二个参数是方差，减均值再除以方差。
#将去0.5再除以0.5，这样图片就转化到-1 - 1之间。如果是三通道，那么transforms.Normalize([a,b,c],[d,c,e])来表示对应通道的均值和方差。

train_dataset = datasets.MNIST(root='.data',train=True,transform=data_tf,download=True)
test_dataset = datasets.MNIST(root='.data',train=False,transform=data_tf)
train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)
test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)

model = Batch_Net(28*28,300,100,10)
if torch.cuda.is_available():
    model = model.cuda()
    
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(),lr=learning_rate)

for epoch in range(num_epoches):
    train_loss = 0
    train_acc = 0
    model.train()
    num = 0
    for data in train_loader:
        img, label =data
        img = img.view(img.size(0),-1)
        if torch.cuda.is_available():
            img = Variable(img).cuda()
            label = Variable(label).cuda()
        else:
            img = Variable(img)
            label = Variable(label)
        out = model(img)
        loss = criterion(out,label)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, pred = out.max(1)
        num_correct = (pred == label).sum().item()
        acc = num_correct / img.shape[0]
        train_acc += acc
        num += 1
    print('epoch:{:d},acc:{:.6f}'.format(epoch,train_acc/num)）
```

![image-20200509155155751](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200509155155751.png)

```python
model.eval()
eval_loss =0
eval_acc = 0
for data in test_loader:
    img,label = data
    img = img.view(img.size(0),-1)
    if torch.cuda.is_available():
        with torch.no_grad():
            img = Variable(img).cuda()  #with torch.no_grad():,表示前向传播不会保留缓存，因为测
            							#试集不需要反向传播，可以节约内存
            label = Variable(label).cuda()
    else:
        with torch.no_grad():
            img = Variable(img)
            label = Variable(label)
    out = model(img)
    loss = criterion(out,label)
    eval_loss += loss.data*label.size(0)
    _,pred = out.max(1)
    num_correct = (pred == label).sum().item()
    eval_acc += num_correct
print('test loss:{:.6f},Acc:{:.6f}'.format(eval_loss/(len(test_dataset)), eval_acc/(len(test_dataset))))
```

![image-20200509155225600](https://raw.githubusercontent.com/shaodongheng/cloudimage/master/img/image-20200509155225600.png)